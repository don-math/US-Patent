{"cells":[{"cell_type":"markdown","metadata":{"id":"NTb5xRgcvG_h","tags":[]},"source":["# Preparing Environment\n","\n","We need to complete the following tasks before analyzing our data.\n","\n","1.   Install and load necssary packages, such as Kaggle, transformers...\n","2.   Check the running environment.\n","3.   Log in the Kaggle API \n","4.   Create a file path.\n","5.   Download datasets and then unzip them into csv format.\n","6.   Finally, check if our datasets are available or not.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHVTWc00Gtcx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669231987461,"user_tz":420,"elapsed":21662,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}},"outputId":"87aaafae-21ba-4dea-968f-a2bf18ff0d71"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 451 kB 31.3 MB/s \n","\u001b[K     |████████████████████████████████| 5.5 MB 72.1 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 76.8 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 87.1 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 89.0 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 88.2 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 65.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 73.1 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q kaggle\n","!pip install -q datasets transformers[sentencepiece]\n","!pip install -q fastai"]},{"cell_type":"code","source":["import warnings\n","warnings.simplefilter('ignore')"],"metadata":{"id":"YsggOgFo2S6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiJMSZ1YG49J"},"outputs":[],"source":["creds = '{\"username\":\"dongdonglu\",\"key\":\"a15c9f012569587f3892a56683059d9a\"}'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwSiYCD9ujHT"},"outputs":[],"source":["from fastai.imports import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mODWBOgGgUJd"},"outputs":[],"source":["import os\n","iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-nUOfjEHBYe"},"outputs":[],"source":["from pathlib import Path\n","\n","cred_path = Path('~/.kaggle/kaggle.json').expanduser()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55lnagncekNg"},"outputs":[],"source":["if not cred_path.exists():\n","  cred_path.parent.mkdir(exist_ok=True)\n","  cred_path.write_text(creds)\n","\n","cred_path.chmod(0o600)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2JUP92ke0WC"},"outputs":[],"source":["path = Path('us-patent-phrase-to-phrase-matching')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCFYcp3de-iX","outputId":"1f657acb-04da-4215-8711-b90ba7e36086","executionInfo":{"status":"ok","timestamp":1669231987811,"user_tz":420,"elapsed":4,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":8}],"source":["path.exists()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3cBy01QfzDW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669231990047,"user_tz":420,"elapsed":2240,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}},"outputId":"0953c2fd-612c-4a6d-a1c2-06afbb0ba24c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading us-patent-phrase-to-phrase-matching.zip to /content\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 682k/682k [00:01<00:00, 568kB/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["if not iskaggle and not path.exists():\n","  import zipfile, kaggle\n","  kaggle.api.competition_download_cli(str(path))\n","  zipfile.ZipFile(f'{path}.zip').extractall(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHp1r_E4f43z","outputId":"3397c3c5-13b5-422f-af81-0e4d70d00233","executionInfo":{"status":"ok","timestamp":1669231990376,"user_tz":420,"elapsed":334,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["sample_submission.csv  test.csv  train.csv\n"]}],"source":["ls {path}"]},{"cell_type":"markdown","metadata":{"id":"R1OyJXeMvM6X"},"source":["# EDA"]},{"cell_type":"markdown","metadata":{"id":"pZHmkp8_vElw"},"source":["We first load our dataset to see what it looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"3j7EBxoOlUYe","outputId":"544b8d9d-3a8b-4b5e-89fd-f3efbe228ef3","executionInfo":{"status":"ok","timestamp":1669231990377,"user_tz":420,"elapsed":6,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     id        anchor                  target context  score\n","0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n","1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n","2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n","3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n","4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n","...                 ...           ...                     ...     ...    ...\n","36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n","36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n","36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n","36471  756ec035e694722b  wood article         wooden material     B44   0.75\n","36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n","\n","[36473 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-4e037b4a-0ddf-404e-95c2-73c5d8c7e53c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36468</th>\n","      <td>8e1386cbefd7f245</td>\n","      <td>wood article</td>\n","      <td>wooden article</td>\n","      <td>B44</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>36469</th>\n","      <td>42d9e032d1cd3242</td>\n","      <td>wood article</td>\n","      <td>wooden box</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36470</th>\n","      <td>208654ccb9e14fa3</td>\n","      <td>wood article</td>\n","      <td>wooden handle</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>36471</th>\n","      <td>756ec035e694722b</td>\n","      <td>wood article</td>\n","      <td>wooden material</td>\n","      <td>B44</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>36472</th>\n","      <td>8d135da0b55b8c88</td>\n","      <td>wood article</td>\n","      <td>wooden substrate</td>\n","      <td>B44</td>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36473 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e037b4a-0ddf-404e-95c2-73c5d8c7e53c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e037b4a-0ddf-404e-95c2-73c5d8c7e53c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e037b4a-0ddf-404e-95c2-73c5d8c7e53c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["import pandas as pd\n","\n","df = pd.read_csv(path/'train.csv')\n","df"]},{"cell_type":"markdown","metadata":{"id":"x-vCoJZ92ZRS"},"source":["Our dataset has 36473 rows, 733 unique anchors, 106 contexts, and 29340 targets. Certain anchors appears very often, for instance the  \"component composite coating\" appears 152 times.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"xStYWhaold6e","outputId":"0c0ac3a9-3caf-468e-f5bb-faef42082e89","executionInfo":{"status":"ok","timestamp":1669231990377,"user_tz":420,"elapsed":5,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                      id                       anchor       target context\n","count              36473                        36473        36473   36473\n","unique             36473                          733        29340     106\n","top     37d61fd2272659b1  component composite coating  composition     H01\n","freq                   1                          152           24    2186"],"text/html":["\n","  <div id=\"df-b9417bce-cb6a-49da-b40b-e5a4cf8a09e4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>36473</td>\n","      <td>36473</td>\n","      <td>36473</td>\n","      <td>36473</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>36473</td>\n","      <td>733</td>\n","      <td>29340</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>component composite coating</td>\n","      <td>composition</td>\n","      <td>H01</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>152</td>\n","      <td>24</td>\n","      <td>2186</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9417bce-cb6a-49da-b40b-e5a4cf8a09e4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b9417bce-cb6a-49da-b40b-e5a4cf8a09e4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b9417bce-cb6a-49da-b40b-e5a4cf8a09e4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}],"source":["df.describe(include='object')\n","# every object is respect to id, anchor, target and context."]},{"cell_type":"markdown","metadata":{"id":"xO4E78aO2_aC"},"source":["Then we went futher to check if the scores, context are balanced or not. It seems that the scores are balanced, but the context is little bit skewed toward lower scores."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"tTqGywf5sxVM","outputId":"5cc81bc4-85c1-4711-fcb7-830598f3f793","executionInfo":{"status":"ok","timestamp":1669231990692,"user_tz":420,"elapsed":320,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f06aed11f50>"]},"metadata":{},"execution_count":13},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["df.score.hist()"]},{"cell_type":"markdown","metadata":{"id":"V9NRaH-I32g8"},"source":["The most frequently appearing context is \"B\" and the least frequently appearing context is \"D\". The sub-context are even more unbalanced, this might be due to the fact that more patents are granted in certain areas than than others."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBU6a2yZrYGG","outputId":"a505e5f2-87ee-4b30-86b5-9cc09c7f3ff4","executionInfo":{"status":"ok","timestamp":1669231990692,"user_tz":420,"elapsed":6,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["B    8019\n","H    6195\n","G    6013\n","C    5288\n","A    4094\n","F    4054\n","E    1531\n","D    1279\n","Name: section, dtype: int64"]},"metadata":{},"execution_count":14}],"source":["df['section'] = df.context.str[0] # get the initial letter for the context\n","df.section.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SP2R2nykrMA4","outputId":"0a4c6c05-aaa8-4d26-bb47-069c8c3e1c4a","executionInfo":{"status":"ok","timestamp":1669231990692,"user_tz":420,"elapsed":5,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["H01    2186\n","H04    2177\n","G01    1812\n","A61    1477\n","F16    1091\n","       ... \n","B03      47\n","F17      33\n","B31      24\n","A62      23\n","F26      18\n","Name: context, Length: 106, dtype: int64"]},"metadata":{},"execution_count":15}],"source":["df.context.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"iGQjiU_45F9e"},"source":["Lastly we check what the instances with the highest and lowest score look like. Notice that \"abatement\" has similarity score 1 with \"abating\" in context \"F\", however it only has similarity score 0 with \"rent abatement\" in context A. This shows that the context is very important in determining the similarity between two words, rather than the simple word-spellings of phrases."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"SGNCi27etIlk","outputId":"fe64b151-4d88-4e8e-c9c7-79dc19b2c199","executionInfo":{"status":"ok","timestamp":1669231990693,"user_tz":420,"elapsed":6,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     id                anchor                     target  \\\n","28     473137168ebf7484             abatement                    abating   \n","158    621b048d70aa8867  absorbent properties  absorbent characteristics   \n","161    bc20a1c961cb073a  absorbent properties      absorption properties   \n","311    e955700dffd68624       acid absorption         absorption of acid   \n","315    3a09aba546aac675       acid absorption            acid absorption   \n","...                 ...                   ...                        ...   \n","36398  913141526432f1d6         wiring trough             wiring troughs   \n","36435  ee0746f2a8ecef97          wood article              wood articles   \n","36440  ecaf479135cf0dfd          wood article             wooden article   \n","36464  8ceaa2b5c2d56250          wood article               wood article   \n","36468  8e1386cbefd7f245          wood article             wooden article   \n","\n","      context  score section  \n","28        F24    1.0       F  \n","158       D01    1.0       D  \n","161       D01    1.0       D  \n","311       B08    1.0       B  \n","315       B08    1.0       B  \n","...       ...    ...     ...  \n","36398     F16    1.0       F  \n","36435     B05    1.0       B  \n","36440     B05    1.0       B  \n","36464     B44    1.0       B  \n","36468     B44    1.0       B  \n","\n","[1154 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-ebb41dc6-cfc4-46da-87c9-1a482662821f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>section</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>28</th>\n","      <td>473137168ebf7484</td>\n","      <td>abatement</td>\n","      <td>abating</td>\n","      <td>F24</td>\n","      <td>1.0</td>\n","      <td>F</td>\n","    </tr>\n","    <tr>\n","      <th>158</th>\n","      <td>621b048d70aa8867</td>\n","      <td>absorbent properties</td>\n","      <td>absorbent characteristics</td>\n","      <td>D01</td>\n","      <td>1.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>161</th>\n","      <td>bc20a1c961cb073a</td>\n","      <td>absorbent properties</td>\n","      <td>absorption properties</td>\n","      <td>D01</td>\n","      <td>1.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>311</th>\n","      <td>e955700dffd68624</td>\n","      <td>acid absorption</td>\n","      <td>absorption of acid</td>\n","      <td>B08</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>315</th>\n","      <td>3a09aba546aac675</td>\n","      <td>acid absorption</td>\n","      <td>acid absorption</td>\n","      <td>B08</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36398</th>\n","      <td>913141526432f1d6</td>\n","      <td>wiring trough</td>\n","      <td>wiring troughs</td>\n","      <td>F16</td>\n","      <td>1.0</td>\n","      <td>F</td>\n","    </tr>\n","    <tr>\n","      <th>36435</th>\n","      <td>ee0746f2a8ecef97</td>\n","      <td>wood article</td>\n","      <td>wood articles</td>\n","      <td>B05</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36440</th>\n","      <td>ecaf479135cf0dfd</td>\n","      <td>wood article</td>\n","      <td>wooden article</td>\n","      <td>B05</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36464</th>\n","      <td>8ceaa2b5c2d56250</td>\n","      <td>wood article</td>\n","      <td>wood article</td>\n","      <td>B44</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36468</th>\n","      <td>8e1386cbefd7f245</td>\n","      <td>wood article</td>\n","      <td>wooden article</td>\n","      <td>B44</td>\n","      <td>1.0</td>\n","      <td>B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1154 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebb41dc6-cfc4-46da-87c9-1a482662821f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ebb41dc6-cfc4-46da-87c9-1a482662821f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ebb41dc6-cfc4-46da-87c9-1a482662821f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}],"source":["df[df['score'] == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"aGUCFMRa5V8o","outputId":"722c1168-dd45-4444-ba48-c0cb2de1bae1","executionInfo":{"status":"ok","timestamp":1669231990693,"user_tz":420,"elapsed":5,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     id        anchor                   target context  score  \\\n","4      54c1e3b9184cb5b6     abatement            forest region     A47    0.0   \n","14     9001756895ec8ca1     abatement    pollution certificate     A47    0.0   \n","15     cc96541d4987b399     abatement           rent abatement     A47    0.0   \n","18     1222e36d9a94c2a4     abatement          stone abutments     A47    0.0   \n","19     a8c9e9f37d4d836a     abatement            tax abatement     A47    0.0   \n","...                 ...           ...                      ...     ...    ...   \n","36447  c7e11e374c52eec4  wood article          article spinner     B44    0.0   \n","36448  c93eebfb43880214  wood article         article spinning     B44    0.0   \n","36449  c997d1dc4fbcd02b  wood article  article tracking system     B44    0.0   \n","36457  1d5bb300f80bc6db  wood article         plastic articles     B44    0.0   \n","36463  16a5c8551e534d1c  wood article         wood apple fruit     B44    0.0   \n","\n","      section  \n","4           A  \n","14          A  \n","15          A  \n","18          A  \n","19          A  \n","...       ...  \n","36447       B  \n","36448       B  \n","36449       B  \n","36457       B  \n","36463       B  \n","\n","[7471 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-51547d75-478d-482a-bf79-37dd01280c02\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>section</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.0</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>9001756895ec8ca1</td>\n","      <td>abatement</td>\n","      <td>pollution certificate</td>\n","      <td>A47</td>\n","      <td>0.0</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>cc96541d4987b399</td>\n","      <td>abatement</td>\n","      <td>rent abatement</td>\n","      <td>A47</td>\n","      <td>0.0</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1222e36d9a94c2a4</td>\n","      <td>abatement</td>\n","      <td>stone abutments</td>\n","      <td>A47</td>\n","      <td>0.0</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>a8c9e9f37d4d836a</td>\n","      <td>abatement</td>\n","      <td>tax abatement</td>\n","      <td>A47</td>\n","      <td>0.0</td>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36447</th>\n","      <td>c7e11e374c52eec4</td>\n","      <td>wood article</td>\n","      <td>article spinner</td>\n","      <td>B44</td>\n","      <td>0.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36448</th>\n","      <td>c93eebfb43880214</td>\n","      <td>wood article</td>\n","      <td>article spinning</td>\n","      <td>B44</td>\n","      <td>0.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36449</th>\n","      <td>c997d1dc4fbcd02b</td>\n","      <td>wood article</td>\n","      <td>article tracking system</td>\n","      <td>B44</td>\n","      <td>0.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36457</th>\n","      <td>1d5bb300f80bc6db</td>\n","      <td>wood article</td>\n","      <td>plastic articles</td>\n","      <td>B44</td>\n","      <td>0.0</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>36463</th>\n","      <td>16a5c8551e534d1c</td>\n","      <td>wood article</td>\n","      <td>wood apple fruit</td>\n","      <td>B44</td>\n","      <td>0.0</td>\n","      <td>B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7471 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51547d75-478d-482a-bf79-37dd01280c02')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-51547d75-478d-482a-bf79-37dd01280c02 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-51547d75-478d-482a-bf79-37dd01280c02');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}],"source":["df[df['score'] == 0]"]},{"cell_type":"markdown","metadata":{"id":"9_QsG-oPvX0a"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7H6Ir4JQY3C"},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer"]},{"cell_type":"markdown","metadata":{"id":"HMBH4ZbS7m1m"},"source":["Here we provide a list of models, which will use in the following evaluations. Why do I pick those models? The answer is pretty straight, because I see other people are getting good results on Kaggle by using these models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luZ3kccwQyBC"},"outputs":[],"source":["model_dbt = 'microsoft/deberta-v3-small' \n","model_bfp = 'anferico/bert-for-patents'"]},{"cell_type":"markdown","metadata":{"id":"Mrd-337U8RKT"},"source":["Write a function to automate the processing of getting the tokenizer corresponding to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAsvGePDSU8E"},"outputs":[],"source":["def tokz(model):\n","  return AutoTokenizer.from_pretrained(model)"]},{"cell_type":"markdown","metadata":{"id":"kgXHNxi83xtW"},"source":["To conveniently convert the problem into a classification problem, we need to merge all the information for each row into a long string. The easiest way to do this is concatenate all the column values with the tokenizer's seperation token [SEP]. There are most sophiscated strategies such as convert the context into their English descriptions and then concatenate which has been proved helpful by others. But for simplicity we will only go with the simpliest method here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["792e38dd4c764bcdb79bce27c758a83c","ef306eea21524f91929322f97384b22a","e8e5ebf2cf6c4b5894aaa48682560fc5","c7489cf97ad04ad5bbcf27d6865974fc","e9ebefbb7258444dab3e2965a8c486ac","fef12060d9e6465aa7ce0c067d4e64dd","52aa6bc29fc44c3bb3e2f4ef95fafe01","2092d0e304b24b0a953c455a821a64a1","692d36d1936d4f90bcffc7f0949a2334","86f93ada9e704297be87a9be952b7e0f","b251ae39e8dd4a4d896e2588966c3314","a0b5e9b4eb2043c7aeced5a78e47da05","afaf0d67c4d14862b32989c045866cbc","2d5207251012429784a30933feaaa21f","937badeda02b43f88ebf4be4e1ffcf31","b2bb22af62af46ceb462503b62150c7f","e8d08cc38b724d6d96328a912396e36e","4f777f397c5d43369f5cf1918d1630ad","9fb0d0ae05974242a26fb475a65b3a81","c883b638ac644600a267d2e994db408d","2525e1c4d4f34069bfa446ff91706c0b","5087c845d3614c2cb8e4745cb85d0f7f","c99f7e6aa04b4432897585737ed4cac5","0085dfac9c6041509b69c35c4bb17425","26848567776c43ffb18dc063a3d38139","5124988af1024c53bb2971f2f2618abc","e9eab37ab65f4708bc7049bb46cd4842","41132e9d6c8c46229fbf2b9b9d1466cc","1e8316b6aa764f5abf32555b3a6664ae","b78ae07c3d9445b599cb7b5c3f704e8f","b163b22fce8e45d18f3edc289bce2482","9ec9dbcf0e78438e99177422362e506d","b18673fdf4d14346bf7d2704b8d8d59e"]},"id":"wotr_cQTiGZY","outputId":"be3d81b1-0c1c-48a7-8ad3-7f855a201295","executionInfo":{"status":"ok","timestamp":1669232002330,"user_tz":420,"elapsed":8368,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792e38dd4c764bcdb79bce27c758a83c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/578 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b5e9b4eb2043c7aeced5a78e47da05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99f7e6aa04b4432897585737ed4cac5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["'[SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["sep = tokz(model_dbt).sep_token\n","sep # same with bert for patent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0GDXTOaian-"},"outputs":[],"source":["df['inputs'] = df.context + sep + df.anchor + sep + df.target "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPjh4iSHP4CB","outputId":"c9d6a37c-8379-40d0-9a76-cfce907db2c1","executionInfo":{"status":"ok","timestamp":1669232002331,"user_tz":420,"elapsed":9,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        A47[SEP]abatement[SEP]abatement of pollution\n","1                A47[SEP]abatement[SEP]act of abating\n","2               A47[SEP]abatement[SEP]active catalyst\n","3           A47[SEP]abatement[SEP]eliminating process\n","4                 A47[SEP]abatement[SEP]forest region\n","                             ...                     \n","36468         B44[SEP]wood article[SEP]wooden article\n","36469             B44[SEP]wood article[SEP]wooden box\n","36470          B44[SEP]wood article[SEP]wooden handle\n","36471        B44[SEP]wood article[SEP]wooden material\n","36472       B44[SEP]wood article[SEP]wooden substrate\n","Name: inputs, Length: 36473, dtype: object"]},"metadata":{},"execution_count":23}],"source":["df.inputs\n","# dataframe can pull out the feature as .input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hly0Q04OVg-1"},"outputs":[],"source":["# eval_df = pd.read_csv(path/'test.csv') # no score provided\n","# eval_df['inputs'] = eval_df.context + sep + eval_df.anchor + sep + eval_df.target "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Txf-jegfy3a"},"outputs":[],"source":["# eval_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSihYdG5f1fq"},"outputs":[],"source":["# eval_df.describe(include='object')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G76MGazLQqIz","outputId":"31ed350c-aba9-4590-c7a4-313c8b0fa3aa","executionInfo":{"status":"ok","timestamp":1669232002332,"user_tz":420,"elapsed":8,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'anchor', 'target', 'context', 'label', 'section', 'inputs'],\n","    num_rows: 36473\n","})"]},"metadata":{},"execution_count":27}],"source":["ds = Dataset.from_pandas(df).rename_column('score', 'label')\n","ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGKiP_x26_jD"},"outputs":[],"source":["# tokz_dbt = tokz(model_dbt)\n","# tokz_bfp = tokz(model_bfp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0x8u4DhuQFV"},"outputs":[],"source":["# def get_eval_ds(model):\n","#   tokenizer = tokz(model)\n","#   def tok_func(x):\n","#     return tokenizer(x['inputs'])\n","#   return Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n","\n","# get_eval_ds(model_dbt) # check if it work"]},{"cell_type":"markdown","metadata":{"id":"KRDwZOP0QHCT"},"source":["To save memory, we create a list of columns to be removed (they will not be taken as inputs for the model)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11xTNh6oTeW9"},"outputs":[],"source":["rm_columns = 'anchor', 'target', 'context', 'id', 'section', 'inputs'"]},{"cell_type":"markdown","metadata":{"id":"xam9HEiPnnVV"},"source":["## Creating Splits"]},{"cell_type":"markdown","metadata":{"id":"eCZVK-4QRv4E"},"source":["According to a Kaggle post, the private test anchors do not overlap with the training set, therefore we should also apply the same practice for our validation and test sets. We first get all the unique anchors, shuffle them and then create indexs for train, validation, and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8UxhzFRnjQG","outputId":"86b3a226-5e81-471c-bb00-09971100b11d","executionInfo":{"status":"ok","timestamp":1669232002332,"user_tz":420,"elapsed":6,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["733"]},"metadata":{},"execution_count":31}],"source":["anchors = df.anchor.unique()\n","len(anchors)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VTlgto6ny8m","outputId":"6dc73364-a779-4272-8154-95d4a9b7f533","executionInfo":{"status":"ok","timestamp":1669232002333,"user_tz":420,"elapsed":6,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['running tally', 'fire cartridges', 'cooled reaction', 'mayenite',\n","       'produce by pump'], dtype=object)"]},"metadata":{},"execution_count":32}],"source":["np.random.seed(1)\n","np.random.shuffle(anchors)\n","anchors[:5] # see 5 anchors"]},{"cell_type":"markdown","metadata":{"id":"BD2mYUS-T1ei"},"source":["Manually set the first 10 anchors as test anchors. The rest 25% (183) of anchors as validation anchors, and all the others (540) as training anchors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrhEF6J7oDV1"},"outputs":[],"source":["val_prop =0.25\n","val_sz = int(len(anchors)*val_prop)\n","\n","test_anchors = anchors[:10]\n","val_anchors = anchors[10:val_sz]\n","train_anchors = anchors[val_sz:]"]},{"cell_type":"markdown","metadata":{"id":"baD8Q7CpUnuW"},"source":["Next, we create the index lists for train, validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeoFsmmAoy7I"},"outputs":[],"source":["# create test indexes\n","test_idxs = [i for i in range(len(df)) if df.anchor[i] in test_anchors]\n","test_idxs = np.array(test_idxs)\n","\n","# create validation indexes\n","val_idxs = [i for i in range(len(df)) if df.anchor[i] in val_anchors]\n","val_idxs = np.array(val_idxs)\n","\n","# create training indexes\n","trn_idxs = [i for i in range(len(df)) if df.anchor[i] not in val_anchors]\n","trn_idxs = np.array(trn_idxs)"]},{"cell_type":"markdown","metadata":{"id":"VsBbxraJXc9H"},"source":["Create a function to prepare the dataset for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96-0EFzxztmK"},"outputs":[],"source":["def get_dds(df, model):\n","  ds = Dataset.from_pandas(df).rename_column('score', 'label')\n","  tokenizer = tokz(model)\n","  def tokz_map(x):\n","    return tokenizer(x['inputs'])\n","  tok_ds = ds.map(tokz_map, batched=True, remove_columns=rm_columns)\n","  return DatasetDict({'train':tok_ds.select(trn_idxs),\n","                      'eval':tok_ds.select(val_idxs), \n","                      'test': tok_ds.select(test_idxs)})"]},{"cell_type":"markdown","metadata":{"id":"RxP9Ah99YL1F"},"source":["Then we create a dataset for test purpose and evaluate the difference between this random split of train and test using their mean scores. It turns out that the difference is negligible."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["eebd933cf23e4c91ad5147d9f0550b3a","0e35a29a85d74501b8368afd86180485","ba186bacf97d4047917fa7b5fc4fc442","a648959982bb4d6cb01df889fc3f9a3b","cf9c356914d54264ab34e1fbfa635ae7","9a988e5d02fd498eba93e787ee35be62","379f4258dcb64108bc73aee4ab0e9524","e0459045a6ea4e3b9255b545b68e0864","9ee76b2b907844378616db3ee22851f3","3af7688d94464e8b8bbed6c1cdd1bdd7","d52bc4cbd1854df686c6e8f224c9f692"]},"id":"4umVeK1IXsqa","outputId":"847fda2e-2c65-4a78-bbf8-d51d10aef205","executionInfo":{"status":"ok","timestamp":1669232007789,"user_tz":420,"elapsed":3409,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebd933cf23e4c91ad5147d9f0550b3a"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(0.36164796156190615, 0.36340866728797766)"]},"metadata":{},"execution_count":36}],"source":["dds_dbt = get_dds(df, model_dbt)\n","\n","from numpy import mean\n","mean(dds_dbt['train']['label']), mean(dds_dbt['eval']['label'])"]},{"cell_type":"markdown","metadata":{"id":"v27-Cg0Ov9Ib"},"source":["# Initiate models"]},{"cell_type":"markdown","metadata":{"id":"7bvfa_h8Y78A"},"source":["Before we initiate our models, we need to define a metric. According to the Kaggle competition's webpage, \"submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.\" So we define Pearson correlation score as the evaluation metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wXJLyluhQoK"},"outputs":[],"source":["import numpy as np\n","def corr(eval_pred): return {'pearson': np.corrcoef(*eval_pred)[0][1]}"]},{"cell_type":"markdown","metadata":{"id":"Hw2ynXjdZv_x"},"source":["Then we create learning parameters that would work well in Colab in experience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xq_hMDQ9i67V"},"outputs":[],"source":["bs = 128\n","epochs = 4\n","wd = 0.01\n","lr = 8e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yCyVyw6hpLj"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer"]},{"cell_type":"markdown","metadata":{"id":"KYkN2AC9e583"},"source":["We also need two functions to accerlerate the creations of datasets, trainers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbE2bjSO2PKb"},"outputs":[],"source":["def get_model(model):\n","  return AutoModelForSequenceClassification.from_pretrained(model, num_labels=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG1sko6WjhuB"},"outputs":[],"source":["def get_trainer(dds, model):\n","  model_trainer = get_model(model) \n","  args = TrainingArguments('outputs', \n","                          learning_rate=lr,\n","                          warmup_ratio=0.1,\n","                          lr_scheduler_type='cosine',\n","                          fp16=True,\n","                          evaluation_strategy='epoch',\n","                          per_device_train_batch_size=bs,\n","                          per_device_eval_batch_size=bs*2,\n","                          num_train_epochs=epochs,\n","                          weight_decay=wd,\n","                          report_to='none')\n","  return Trainer(model_trainer, args, train_dataset=dds['train'], \n","                 eval_dataset=dds['eval'], \n","                 tokenizer=tokz(model), \n","                 compute_metrics=corr)"]},{"cell_type":"markdown","metadata":{"id":"RQdRy6-WbQ70"},"source":["## DeBERTa"]},{"cell_type":"markdown","metadata":{"id":"aDFkDEZIglZv"},"source":["According to the [original paper](https://arxiv.org/abs/2006.03654) \"DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%)\"."]},{"cell_type":"markdown","metadata":{"id":"Dt5nXWT7hRFO"},"source":["First we initiate dataset and trainer for DeBERTa, then we start the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140,"referenced_widgets":["424078f2bd6d47589ed69d49227c7181","ba9fc9e390a84126a9a1bc56c790b704","57a274a367fa47adb6a5ce07e4262370","d9222d649b3f453eba6937d4311914f5","d263504aef444faabb8cf50164d310c3","f0453807745f4b28819efa511d3e5eae","58c08bfc34e34fd1a4273c75c88558d4","5eef78f8fe7b45cebfa17dd7bed5b23e","f247415d8a034ab59f463c24fab8f9d0","594a78b41eb34ac8b1c6df5827495566","91296adfaf59460985ad9d2686a0429e","bcc5bc784a87462693412156add9a636","a41479c5353c402e8a3ca5be2cbd2823"]},"id":"7PhJNjk43a0p","outputId":"fa164df6-a89a-4dfa-c81a-98307b615bdf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bcc5bc784a87462693412156add9a636","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a41479c5353c402e8a3ca5be2cbd2823","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/273M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 27889\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 872\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='872' max='872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [872/872 05:17, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.028168</td>\n","      <td>0.773382</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.027317</td>\n","      <td>0.785380</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.037600</td>\n","      <td>0.026905</td>\n","      <td>0.789747</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.037600</td>\n","      <td>0.026486</td>\n","      <td>0.790443</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 8584\n","  Batch size = 256\n","***** Running Evaluation *****\n","  Num examples = 8584\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 8584\n","  Batch size = 256\n","***** Running Evaluation *****\n","  Num examples = 8584\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=872, training_loss=0.027323976569219467, metrics={'train_runtime': 318.6939, 'train_samples_per_second': 350.041, 'train_steps_per_second': 2.736, 'total_flos': 476860846159170.0, 'train_loss': 0.027323976569219467, 'epoch': 4.0})"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["dds_dbt = get_dds(df, model_dbt);\n","trainer_dbt = get_trainer(dds_dbt, model_dbt);\n","trainer_dbt.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxpV01iIJUFg","outputId":"61cfbb27-d171-4d3c-9870-247c4512dd18"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 395\n","})"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["ds_test_dbt = dds_dbt['test']\n","ds_test_dbt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wphp14j1-N7e","outputId":"577d7612-baf9-4691-f5a0-72437d22a751"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 395\n","  Batch size = 256\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='547' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 10:02]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["array([0.        , 0.        , 0.55322266, 0.        , 0.6064453 ,\n","       0.016922  , 0.20275879, 0.6894531 , 0.02111816, 0.23876953,\n","       0.29052734, 0.6669922 , 0.        , 0.30273438, 0.42285156,\n","       0.36035156, 0.3190918 , 0.19482422, 0.36889648, 0.4428711 ,\n","       0.24829102, 0.5732422 , 0.31445312, 0.4650879 , 0.5463867 ,\n","       0.1595459 , 0.5083008 , 0.21374512, 0.32641602, 0.64746094,\n","       0.        , 0.7138672 , 0.6269531 , 0.        , 0.        ,\n","       0.6411133 , 0.42138672, 0.2927246 , 0.46264648, 0.03283691,\n","       0.2133789 , 0.01885986, 0.4482422 , 0.55371094, 0.5083008 ,\n","       0.53759766, 0.49438477, 0.61816406, 0.26293945, 0.31689453,\n","       0.30029297, 0.7138672 , 0.69970703, 0.01617432, 0.7426758 ,\n","       0.        , 0.        , 0.        , 0.4868164 , 0.3046875 ,\n","       0.4645996 , 0.3178711 , 0.4675293 , 0.        , 0.3022461 ,\n","       0.6088867 , 0.68408203, 0.2541504 , 0.0559082 , 0.52246094,\n","       0.        , 0.53564453, 0.        , 0.57714844, 0.25854492,\n","       0.29223633, 0.29296875, 0.47045898, 0.75      , 0.37646484,\n","       0.05865479, 0.        , 0.60253906, 0.23742676, 0.3413086 ,\n","       0.3564453 , 0.        , 0.35058594, 0.2980957 , 0.5214844 ,\n","       0.76904297, 0.11767578, 0.        , 0.0791626 , 0.3959961 ,\n","       0.32250977, 0.22338867, 0.21716309, 0.31201172, 0.32128906,\n","       0.32714844, 0.1348877 , 0.453125  , 0.5761719 , 0.55859375,\n","       0.        , 0.24108887, 0.3071289 , 0.06167603, 0.26123047,\n","       0.3791504 , 0.65722656, 0.6586914 , 0.        , 0.5019531 ,\n","       0.5366211 , 0.        , 0.        , 0.3996582 , 0.36572266,\n","       0.55322266, 0.4716797 , 0.24975586, 0.6694336 , 0.6386719 ,\n","       1.        , 0.5883789 , 0.29125977, 0.11193848, 0.15234375,\n","       0.6376953 , 0.40454102, 0.0463562 , 0.41577148, 0.7246094 ,\n","       0.00376511, 0.58154297, 0.57373047, 0.43774414, 0.        ,\n","       0.6923828 , 0.6640625 , 0.6748047 , 0.49853516, 0.5488281 ,\n","       0.41357422, 0.5703125 , 0.25585938, 0.6489258 , 0.        ,\n","       0.        , 0.        , 0.        , 0.5239258 , 0.36254883,\n","       0.39160156, 0.26367188, 0.        , 0.3630371 , 0.25512695,\n","       0.43139648, 0.4868164 , 0.24389648, 0.26513672, 0.        ,\n","       0.04840088, 0.        , 0.        , 0.2548828 , 0.4440918 ,\n","       0.42529297, 0.6010742 , 0.8076172 , 0.49389648, 0.        ,\n","       0.1182251 , 0.5957031 , 0.50683594, 0.        , 0.71240234,\n","       0.58691406, 0.5756836 , 0.31347656, 0.41064453, 0.45263672,\n","       0.83496094, 0.8261719 , 0.40161133, 0.22717285, 0.        ,\n","       0.50878906, 0.        , 0.46484375, 0.2208252 , 0.        ,\n","       0.22424316, 0.08221436, 0.20227051, 0.19299316, 0.20373535,\n","       0.19689941, 0.09881592, 0.13574219, 0.39282227, 0.5126953 ,\n","       0.5986328 , 1.        , 0.7792969 , 0.65966797, 0.7832031 ,\n","       0.66503906, 0.        , 0.34936523, 0.0368042 , 0.26171875,\n","       0.27197266, 0.7426758 , 0.21276855, 0.16760254, 0.20031738,\n","       0.2052002 , 0.2364502 , 0.2590332 , 0.3466797 , 0.42114258,\n","       0.46606445, 0.2800293 , 0.34375   , 0.4416504 , 0.8857422 ,\n","       0.5805664 , 0.43188477, 0.42114258, 0.28955078, 0.32836914,\n","       0.24206543, 0.25463867, 0.29638672, 0.38256836, 0.52246094,\n","       0.40893555, 0.4074707 , 0.4182129 , 0.34814453, 0.55908203,\n","       0.5395508 , 0.49926758, 0.48168945, 0.00228691, 0.40625   ,\n","       0.07672119, 0.54248047, 0.45581055, 0.6147461 , 0.09710693,\n","       0.        , 0.35498047, 0.12432861, 0.35107422, 0.27246094,\n","       0.31152344, 0.31396484, 0.54589844, 0.5       , 0.6328125 ,\n","       0.5395508 , 0.40429688, 0.88378906, 0.65478516, 0.8510742 ,\n","       0.64746094, 0.6665039 , 0.6772461 , 0.6958008 , 0.5341797 ,\n","       0.20776367, 0.        , 0.5751953 , 0.51220703, 0.32006836,\n","       0.3400879 , 0.27929688, 0.33032227, 0.30395508, 0.64208984,\n","       0.2836914 , 0.38061523, 0.32080078, 0.23181152, 0.7651367 ,\n","       0.7988281 , 0.22253418, 0.22436523, 0.31152344, 0.8154297 ,\n","       0.6308594 , 1.        , 0.5336914 , 0.        , 0.5727539 ,\n","       0.4411621 , 0.        , 0.05413818, 1.        , 0.        ,\n","       0.        , 0.49145508, 0.29370117, 0.2322998 , 0.7939453 ,\n","       0.        , 0.96777344, 0.54785156, 0.5571289 , 0.3347168 ,\n","       0.30322266, 0.        , 0.29760742, 0.47314453, 0.49487305,\n","       0.5214844 , 0.28344727, 0.25976562, 0.        , 0.        ,\n","       0.2475586 , 0.5678711 , 0.5253906 , 1.        , 0.6323242 ,\n","       0.        , 0.        , 0.        , 0.52490234, 1.        ,\n","       1.        , 1.        , 0.5102539 , 0.24743652, 0.31298828,\n","       0.        , 0.6323242 , 0.16687012, 0.53027344, 0.23522949,\n","       0.1730957 , 0.27856445, 0.28881836, 0.27807617, 0.22424316,\n","       0.32836914, 0.6298828 , 0.33032227, 0.19519043, 0.7158203 ,\n","       0.30078125, 0.23693848, 0.        , 0.14709473, 0.4494629 ,\n","       0.6953125 , 0.5004883 , 0.47998047, 0.5913086 , 0.46777344,\n","       0.02024841, 0.671875  , 0.55078125, 0.        , 0.        ,\n","       0.44506836, 0.5932617 , 0.5151367 , 0.08581543, 0.10943604,\n","       0.546875  , 0.06222534, 0.53808594, 0.30273438, 0.17993164,\n","       0.5595703 , 0.05535889, 0.28515625, 0.47387695, 0.04879761,\n","       0.515625  , 0.52441406, 0.        , 0.02742004, 0.47460938,\n","       0.        , 1.        , 0.2487793 , 0.54052734, 0.4790039 ],\n","      dtype=float32)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["def get_test_predictions(ds, trainer):\n","    preds= trainer.predict(ds)\n","    preds_values = preds.predictions\n","    preds_values_clipped = np.clip(preds_values, 0, 1)\n","    return preds_values_clipped\n","\n","get_test_predictions(ds_test_dbt, trainer_dbt)"]},{"cell_type":"markdown","metadata":{"id":"8JoO9sIAX09h","tags":[]},"source":["### Error Analysis"]},{"cell_type":"markdown","metadata":{"id":"0mIEbzvm-N7e"},"source":["We have trained our first model on the training dataset using DeBERTa model. Now we can use this trained model to check where our highest losses come from. Potentially we can find the mislabelled data in the original datasets or get insights on how we can improve on our model. Let's first create predictions on the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTBau5BSc3N5","outputId":"6994b773-f916-4b33-b4a6-a1c1de0d9a5b"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 27889\n","  Batch size = 256\n"]}],"source":["df = dds_dbt['train']\n","preds = get_test_predictions(df, trainer_dbt)"]},{"cell_type":"markdown","metadata":{"id":"bgdnsuiodCPZ"},"source":["Then we add the predictions into our dataset, convert it into pandas format, calculate and save the losses. We also create temproray pandas df naming ds_train to better play with the data later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5VU_qFxe2cI","outputId":"715595de-dbf3-4e6f-bc14-de4330efd5d8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b69d5fdcd4b84a528861f1ad9b531782","version_major":2,"version_minor":0},"text/plain":["Flattening the indices:   0%|          | 0/28 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>input_ids</th>\n","      <th>token_type_ids</th>\n","      <th>attention_mask</th>\n","      <th>preds</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.50</td>\n","      <td>[1, 736, 3304, 2, 14478, 1022, 2, 14478, 1022, 514, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.586426</td>\n","      <td>0.086426</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.25</td>\n","      <td>[1, 736, 3304, 2, 14478, 1022, 2, 4389, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.277588</td>\n","      <td>0.027588</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.25</td>\n","      <td>[1, 736, 3304, 2, 14478, 1022, 2, 2100, 1022, 1569, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.368896</td>\n","      <td>0.118896</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.25</td>\n","      <td>[1, 736, 3304, 2, 14478, 1022, 2, 1706, 8363, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.268555</td>\n","      <td>0.018555</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.25</td>\n","      <td>[1, 736, 3304, 2, 14478, 1022, 2, 1706, 1569, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.337158</td>\n","      <td>0.087158</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27884</th>\n","      <td>1.00</td>\n","      <td>[1, 736, 5153, 2, 1847, 1030, 2, 4164, 1030, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>27885</th>\n","      <td>0.50</td>\n","      <td>[1, 736, 5153, 2, 1847, 1030, 2, 4164, 1352, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.576660</td>\n","      <td>0.076660</td>\n","    </tr>\n","    <tr>\n","      <th>27886</th>\n","      <td>0.50</td>\n","      <td>[1, 736, 5153, 2, 1847, 1030, 2, 4164, 1997, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.577148</td>\n","      <td>0.077148</td>\n","    </tr>\n","    <tr>\n","      <th>27887</th>\n","      <td>0.75</td>\n","      <td>[1, 736, 5153, 2, 1847, 1030, 2, 4164, 1146, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.808105</td>\n","      <td>0.058105</td>\n","    </tr>\n","    <tr>\n","      <th>27888</th>\n","      <td>0.50</td>\n","      <td>[1, 736, 5153, 2, 1847, 1030, 2, 4164, 11856, 2]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n","      <td>0.585938</td>\n","      <td>0.085938</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27889 rows × 6 columns</p>\n","</div>"],"text/plain":["       label                                               input_ids  \\\n","0       0.50  [1, 736, 3304, 2, 14478, 1022, 2, 14478, 1022, 514, 2]   \n","1       0.25              [1, 736, 3304, 2, 14478, 1022, 2, 4389, 2]   \n","2       0.25  [1, 736, 3304, 2, 14478, 1022, 2, 2100, 1022, 1569, 2]   \n","3       0.25        [1, 736, 3304, 2, 14478, 1022, 2, 1706, 8363, 2]   \n","4       0.25        [1, 736, 3304, 2, 14478, 1022, 2, 1706, 1569, 2]   \n","...      ...                                                     ...   \n","27884   1.00         [1, 736, 5153, 2, 1847, 1030, 2, 4164, 1030, 2]   \n","27885   0.50         [1, 736, 5153, 2, 1847, 1030, 2, 4164, 1352, 2]   \n","27886   0.50         [1, 736, 5153, 2, 1847, 1030, 2, 4164, 1997, 2]   \n","27887   0.75         [1, 736, 5153, 2, 1847, 1030, 2, 4164, 1146, 2]   \n","27888   0.50        [1, 736, 5153, 2, 1847, 1030, 2, 4164, 11856, 2]   \n","\n","                          token_type_ids                     attention_mask  \\\n","0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","1            [0, 0, 0, 0, 0, 0, 0, 0, 0]        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","4         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","...                                  ...                                ...   \n","27884     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","27885     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","27886     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","27887     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","27888     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n","\n","          preds      loss  \n","0      0.586426  0.086426  \n","1      0.277588  0.027588  \n","2      0.368896  0.118896  \n","3      0.268555  0.018555  \n","4      0.337158  0.087158  \n","...         ...       ...  \n","27884  1.000000  0.000000  \n","27885  0.576660  0.076660  \n","27886  0.577148  0.077148  \n","27887  0.808105  0.058105  \n","27888  0.585938  0.085938  \n","\n","[27889 rows x 6 columns]"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["df = df.add_column('preds', preds)\n","df.set_format('pandas')\n","df_loss = abs(df['label'] - df['preds'])\n","df = df.add_column('loss', df_loss)\n","\n","ds_train = df[:]\n","ds_train"]},{"cell_type":"markdown","metadata":{"id":"aRW-Gxwv-N7e"},"source":["Here, we sort the losses and save only 4 columns from the training dataset. To see the result, we also need to convert input_ids back to tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGstEJXSrGsA","outputId":"7dba042e-2fbf-4e05-8a28-2836b803032b"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["ds_train = ds_train.sort_values(\"loss\", ascending=False)[['input_ids', 'label', 'preds', 'loss']]\n","\n","tokz_dbt = tokz(model_dbt)\n","ds_train['inputs'] = ds_train['input_ids'].map(lambda x: tokz_dbt.convert_tokens_to_string(tokz_dbt.convert_ids_to_tokens(x)))"]},{"cell_type":"markdown","metadata":{"id":"Zd3MGKDl-N7f"},"source":["Now we can eyeball some big losses come from mis-labels or controvosial labels. Just use our common sense, microchambers and microvessels have some similarity with each other, however they are labled with a score 0. Same reasoning applys to pictorial image and pictorial representation. \n","\n","On the other hand, we do find certain weakness of our model. For instance, Channel vectors are type of vectors, while channels of vectors are type of channels. Our model cannot distinguish this basic fact...To compensate this weakness, we might need to add an additional function to handle this kind of special situations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o28kGKAWnUXX","outputId":"fbca1b11-b78c-4202-f057-4ec0bb1fbe3e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>label</th>\n","      <th>preds</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20528</th>\n","      <td>[CLS] B21[SEP] punch face[SEP] face punch[SEP]</td>\n","      <td>0.75</td>\n","      <td>0.000000</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>18712</th>\n","      <td>[CLS] G09[SEP] pictorial image[SEP] pictorial representation[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.710449</td>\n","      <td>0.710449</td>\n","    </tr>\n","    <tr>\n","      <th>3202</th>\n","      <td>[CLS] F03[SEP] catching surface[SEP] catching nose[SEP]</td>\n","      <td>0.75</td>\n","      <td>0.204102</td>\n","      <td>0.545898</td>\n","    </tr>\n","    <tr>\n","      <th>25267</th>\n","      <td>[CLS] C07[SEP] therapeutic immune[SEP] therapeutic immune therapeutic[SEP]</td>\n","      <td>0.25</td>\n","      <td>0.795410</td>\n","      <td>0.545410</td>\n","    </tr>\n","    <tr>\n","      <th>7881</th>\n","      <td>[CLS] G01[SEP] ecn[SEP] networking system[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.530273</td>\n","      <td>0.530273</td>\n","    </tr>\n","    <tr>\n","      <th>3624</th>\n","      <td>[CLS] H04[SEP] channel vectors[SEP] channels of vectors[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.528809</td>\n","      <td>0.528809</td>\n","    </tr>\n","    <tr>\n","      <th>12831</th>\n","      <td>[CLS] G01[SEP] inner fluid conduit[SEP] inner fluid conduit sufficient[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.522461</td>\n","      <td>0.522461</td>\n","    </tr>\n","    <tr>\n","      <th>16279</th>\n","      <td>[CLS] G01[SEP] noncollinear[SEP] non collinear lines[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.513672</td>\n","      <td>0.513672</td>\n","    </tr>\n","    <tr>\n","      <th>3619</th>\n","      <td>[CLS] H04[SEP] channel vectors[SEP] channel of vectors[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.504883</td>\n","      <td>0.504883</td>\n","    </tr>\n","    <tr>\n","      <th>19798</th>\n","      <td>[CLS] G01[SEP] previously captured image[SEP] previously reference image[SEP]</td>\n","      <td>0.00</td>\n","      <td>0.503906</td>\n","      <td>0.503906</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                              inputs  \\\n","20528                                 [CLS] B21[SEP] punch face[SEP] face punch[SEP]   \n","18712              [CLS] G09[SEP] pictorial image[SEP] pictorial representation[SEP]   \n","3202                         [CLS] F03[SEP] catching surface[SEP] catching nose[SEP]   \n","25267     [CLS] C07[SEP] therapeutic immune[SEP] therapeutic immune therapeutic[SEP]   \n","7881                                  [CLS] G01[SEP] ecn[SEP] networking system[SEP]   \n","3624                    [CLS] H04[SEP] channel vectors[SEP] channels of vectors[SEP]   \n","12831    [CLS] G01[SEP] inner fluid conduit[SEP] inner fluid conduit sufficient[SEP]   \n","16279                      [CLS] G01[SEP] noncollinear[SEP] non collinear lines[SEP]   \n","3619                     [CLS] H04[SEP] channel vectors[SEP] channel of vectors[SEP]   \n","19798  [CLS] G01[SEP] previously captured image[SEP] previously reference image[SEP]   \n","\n","       label     preds      loss  \n","20528   0.75  0.000000  0.750000  \n","18712   0.00  0.710449  0.710449  \n","3202    0.75  0.204102  0.545898  \n","25267   0.25  0.795410  0.545410  \n","7881    0.00  0.530273  0.530273  \n","3624    0.00  0.528809  0.528809  \n","12831   0.00  0.522461  0.522461  \n","16279   0.00  0.513672  0.513672  \n","3619    0.00  0.504883  0.504883  \n","19798   0.00  0.503906  0.503906  "]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["ds_train[['inputs', 'label', 'preds', 'loss']][:10]"]},{"cell_type":"markdown","metadata":{"id":"1wJiDnZf-N7f"},"source":["Since we are supposed to classify the similaries into 5 categories (0.0, 0.25, 0.50, 0.75, 1.0). Let's plot a confusion matrix to see if there is any significant patterns among the mis-labellings. To do this, let's first write a function to round the predictions into its near quarter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UfebAaeh2h0","outputId":"8ffd2fec-1ce0-43f3-87a1-771b44ae653a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input_ids</th>\n","      <th>label</th>\n","      <th>preds</th>\n","      <th>loss</th>\n","      <th>inputs</th>\n","      <th>y_preds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20528</th>\n","      <td>[1, 736, 2917, 2, 8460, 812, 2, 812, 8460, 2]</td>\n","      <td>0.75</td>\n","      <td>0.000000</td>\n","      <td>0.750000</td>\n","      <td>[CLS] B21[SEP] punch face[SEP] face punch[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>18712</th>\n","      <td>[1, 1098, 4505, 2, 38596, 1115, 2, 38596, 5190, 2]</td>\n","      <td>0.00</td>\n","      <td>0.710449</td>\n","      <td>0.710449</td>\n","      <td>[CLS] G09[SEP] pictorial image[SEP] pictorial representation[SEP]</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>3202</th>\n","      <td>[1, 1107, 3984, 2, 8129, 1694, 2, 8129, 4752, 2]</td>\n","      <td>0.75</td>\n","      <td>0.204102</td>\n","      <td>0.545898</td>\n","      <td>[CLS] F03[SEP] catching surface[SEP] catching nose[SEP]</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>25267</th>\n","      <td>[1, 716, 4649, 2, 8068, 5476, 2, 8068, 5476, 8068, 2]</td>\n","      <td>0.25</td>\n","      <td>0.795410</td>\n","      <td>0.545410</td>\n","      <td>[CLS] C07[SEP] therapeutic immune[SEP] therapeutic immune therapeutic[SEP]</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>7881</th>\n","      <td>[1, 1098, 3085, 2, 865, 26478, 2, 5523, 492, 2]</td>\n","      <td>0.00</td>\n","      <td>0.530273</td>\n","      <td>0.530273</td>\n","      <td>[CLS] G01[SEP] ecn[SEP] networking system[SEP]</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22471</th>\n","      <td>[1, 1107, 3085, 2, 6699, 3261, 2, 5542, 3261, 2]</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>[CLS] F01[SEP] seal teeth[SEP] artificial teeth[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>22470</th>\n","      <td>[1, 336, 8540, 2, 6782, 8410, 2, 2014, 4912, 2]</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>[CLS] A63[SEP] scratch coating[SEP] winter coat[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>13326</th>\n","      <td>[1, 1107, 5920, 2, 40006, 6005, 2, 1376, 24422, 6005, 2]</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>[CLS] F41[SEP] intruder detection[SEP] covid detection[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2174</th>\n","      <td>[1, 1107, 4159, 2, 636, 62766, 2, 1191, 636, 514, 2]</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>[CLS] F02[SEP] based propellant[SEP] network based data[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>8115</th>\n","      <td>[1, 336, 8848, 2, 2828, 946, 53836, 2, 4436, 17507, 2]</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>[CLS] A61[SEP] electric field intensities[SEP] frequency lesions[SEP]</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27889 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                      input_ids  label  \\\n","20528             [1, 736, 2917, 2, 8460, 812, 2, 812, 8460, 2]   0.75   \n","18712        [1, 1098, 4505, 2, 38596, 1115, 2, 38596, 5190, 2]   0.00   \n","3202           [1, 1107, 3984, 2, 8129, 1694, 2, 8129, 4752, 2]   0.75   \n","25267     [1, 716, 4649, 2, 8068, 5476, 2, 8068, 5476, 8068, 2]   0.25   \n","7881            [1, 1098, 3085, 2, 865, 26478, 2, 5523, 492, 2]   0.00   \n","...                                                         ...    ...   \n","22471          [1, 1107, 3085, 2, 6699, 3261, 2, 5542, 3261, 2]   0.00   \n","22470           [1, 336, 8540, 2, 6782, 8410, 2, 2014, 4912, 2]   0.00   \n","13326  [1, 1107, 5920, 2, 40006, 6005, 2, 1376, 24422, 6005, 2]   0.00   \n","2174       [1, 1107, 4159, 2, 636, 62766, 2, 1191, 636, 514, 2]   0.00   \n","8115     [1, 336, 8848, 2, 2828, 946, 53836, 2, 4436, 17507, 2]   0.00   \n","\n","          preds      loss  \\\n","20528  0.000000  0.750000   \n","18712  0.710449  0.710449   \n","3202   0.204102  0.545898   \n","25267  0.795410  0.545410   \n","7881   0.530273  0.530273   \n","...         ...       ...   \n","22471  0.000000  0.000000   \n","22470  0.000000  0.000000   \n","13326  0.000000  0.000000   \n","2174   0.000000  0.000000   \n","8115   0.000000  0.000000   \n","\n","                                                                           inputs  \\\n","20528                              [CLS] B21[SEP] punch face[SEP] face punch[SEP]   \n","18712           [CLS] G09[SEP] pictorial image[SEP] pictorial representation[SEP]   \n","3202                      [CLS] F03[SEP] catching surface[SEP] catching nose[SEP]   \n","25267  [CLS] C07[SEP] therapeutic immune[SEP] therapeutic immune therapeutic[SEP]   \n","7881                               [CLS] G01[SEP] ecn[SEP] networking system[SEP]   \n","...                                                                           ...   \n","22471                        [CLS] F01[SEP] seal teeth[SEP] artificial teeth[SEP]   \n","22470                        [CLS] A63[SEP] scratch coating[SEP] winter coat[SEP]   \n","13326                 [CLS] F41[SEP] intruder detection[SEP] covid detection[SEP]   \n","2174                 [CLS] F02[SEP] based propellant[SEP] network based data[SEP]   \n","8115        [CLS] A61[SEP] electric field intensities[SEP] frequency lesions[SEP]   \n","\n","       y_preds  \n","20528     0.00  \n","18712     0.75  \n","3202      0.25  \n","25267     0.75  \n","7881      0.50  \n","...        ...  \n","22471     0.00  \n","22470     0.00  \n","13326     0.00  \n","2174      0.00  \n","8115      0.00  \n","\n","[27889 rows x 6 columns]"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["def round_to_quarter(x):\n","  x_times4 = 4 * x\n","  x_time4_rounded = round(x_times4)\n","  x_rtq = x_time4_rounded / 4\n","  return x_rtq\n","\n","ds_train['y_preds'] = ds_train['preds'].map(lambda x: round_to_quarter(x))\n","ds_train"]},{"cell_type":"markdown","metadata":{"id":"ERvPab5dYwJq"},"source":["Then we can extract predictions and labels, and convert their values into strings so that they can be used as inputs for the confusion matrix (which only take strings are inputs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CH3e9_5EWboy","outputId":"507a1dd8-cb13-4457-b888-aa03b5464b4f"},"outputs":[{"data":{"text/plain":["(['0.0', '0.75', '0.25', '0.75', '0.5'],\n"," ['0.75', '0.0', '0.75', '0.25', '0.0'])"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["y_preds = ds_train['y_preds']\n","y_valid = ds_train['label']\n","\n","y_preds = [str(y_pred) for y_pred in y_preds]\n","y_valid = list(map(str, y_valid))\n","\n","y_preds[:5], y_valid[:5]"]},{"cell_type":"markdown","metadata":{"id":"aR2SueA9-N7f"},"source":["After plotting the confusion matrix, we can see that most of the model's mistakes come from the middle values where the labelled similarity scores range from 0.25 to 0.75. This is plausible as even human disagree with each other on two similar phrases if they were somewhat similar. The model is doing a good job in non-similar (score 0.0) or very-similar (score 1.0) phrases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_8Uy2mk6gbk","outputId":"2172bfaa-3052-48b9-a39d-e91014c5f886"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","\n","labels = [0, 0.25, 0.5, 0.75, 1]\n","\n","\n","def plot_confusion_matrix(y_preds, y_true, labels):\n","    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","    fig, ax = plt.subplots(figsize=(5, 5))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","    plt.title(\"Normalized confusion matrix\")\n","    plt.show()\n","    \n","\n","\n","plot_confusion_matrix(y_preds, y_valid, labels)"]},{"cell_type":"markdown","metadata":{"id":"r5L8h2mR_JP6"},"source":["### Cross-validation"]},{"cell_type":"markdown","metadata":{"id":"8OwrINr4-N7g"},"source":["To see the a more realistic performance of the DeBERTa model on previously unseen data, we need to do cross-validations. First we load the dataset again (but shuffled). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BimYuheq-N7g"},"outputs":[],"source":["df = pd.read_csv(path/'train.csv').sample(frac=1, random_state=1)\n","df['inputs'] = df.context + sep + df.anchor + sep + df.target \n","df['section'] = df.context.str[0]\n","ds = Dataset.from_pandas(df).rename_column('score', 'label')"]},{"cell_type":"markdown","metadata":{"id":"lxV403MM-N7g"},"source":["Then we create a 'get_tok_ds()' function to facilitate the generation of the tokenized dataset with respect to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1tvuJhVAegB","colab":{"referenced_widgets":["ca5d8492da7840c1948754a5ec72fde7"]},"outputId":"d549f6c6-ef50-422d-cff8-8da1b8148eb7"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca5d8492da7840c1948754a5ec72fde7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# shuffle dataframe\n","def get_tok_ds(model):\n","    tokenizer = tokz(model)\n","    def tokz_map(x):\n","        return tokenizer(x['inputs'])\n","    tok_ds = ds.map(tokz_map, batched=True, remove_columns=rm_columns)\n","    return tok_ds\n","\n","tok_ds_dbt = get_tok_ds(model_dbt);"]},{"cell_type":"markdown","metadata":{"id":"gayjNICY-N7g"},"source":["The sklearn offers a train_test_split method which takes a random subset of the data. But this is a poor choice for many real-world problems, since real-world's datasets are often unbalanced w.r.t labels. To create a more balanced split, we use the StatifiedGroupKFold which create folds which preserve the percentage of samples for each class as much as possible given the constraint of non-overlapping groups between splits. For conveniency, we split the datasets into 4 folds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZuquuOk_Oe4"},"outputs":[],"source":["from sklearn.model_selection import StratifiedGroupKFold\n","n_folds = 4 \n","cv = StratifiedGroupKFold(n_splits=n_folds)"]},{"cell_type":"markdown","metadata":{"id":"rY3rXu2C-N7g"},"source":["Then we create an array of indexs of rows, then create the folds w.r.t their scores and anchors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7fmhL-kANff"},"outputs":[],"source":["idxs = np.arange(len(df))\n","scores = (df.score*100).astype(int)\n","folds = list(cv.split(idxs, scores, df.anchor))\n","\n","folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbCYaVz1-N7h"},"outputs":[],"source":["We also need another function to help us get the datasets after splits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngjAJ2BPA3i1"},"outputs":[],"source":["def get_fold(folds, fold_num, model):\n","  train, eval = folds[fold_num]\n","  tok_ds = get_tok_ds(model)\n","  return DatasetDict({'train': tok_ds.select(train), \n","                      'eval': tok_ds.select(eval)})"]},{"cell_type":"markdown","metadata":{"id":"2aA4B_8w-N7h"},"source":["Lastly, we start the CV process and save the results into a list naming \"cv_pearson_dbt\". As we can the Pearson scores are around 0.79-0.80. So if we only use this model without ensembling, this will likely to be our final model performance. However, the highest score on private leadership board is around 0.89, with the assistance ensembling methods. We will also explore the use ensembling in the Combine Models section."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["2e82ad0e35e842178259e95c99481957","b343c27dd8534e5f85c42b185b487488","f35a58232cc54e5ea885ff98c0b695d0","b3c6a7b605d14ab2850c76990f3ce7dd"]},"id":"ckaqdCLO-N7h","outputId":"3f475d7f-5f55-46d0-a18f-ae4d458929d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e82ad0e35e842178259e95c99481957","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n","Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 27346\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 856\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [856/856 05:17, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.027792</td>\n","      <td>0.785831</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.026093</td>\n","      <td>0.797625</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.032900</td>\n","      <td>0.026578</td>\n","      <td>0.798426</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.032900</td>\n","      <td>0.026201</td>\n","      <td>0.799873</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9127\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9127\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9127\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9127\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b343c27dd8534e5f85c42b185b487488","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n","Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 27368\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 856\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [856/856 05:46, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.030275</td>\n","      <td>0.759918</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.027169</td>\n","      <td>0.782810</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.031800</td>\n","      <td>0.027362</td>\n","      <td>0.788240</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.031800</td>\n","      <td>0.027466</td>\n","      <td>0.789437</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9105\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9105\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9105\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9105\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f35a58232cc54e5ea885ff98c0b695d0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n","Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 27351\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 856\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [856/856 05:46, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.032727</td>\n","      <td>0.770930</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.027987</td>\n","      <td>0.783872</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.031900</td>\n","      <td>0.027574</td>\n","      <td>0.789441</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.031900</td>\n","      <td>0.027492</td>\n","      <td>0.788000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9122\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9122\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9122\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9122\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3c6a7b605d14ab2850c76990f3ce7dd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n","Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Adding [MASK] to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n","Model config DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 27354\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 856\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [856/856 05:54, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Pearson</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.027587</td>\n","      <td>0.781386</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.025210</td>\n","      <td>0.800806</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.032800</td>\n","      <td>0.024475</td>\n","      <td>0.802738</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.032800</td>\n","      <td>0.025820</td>\n","      <td>0.800310</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 256\n","Saving model checkpoint to outputs/checkpoint-500\n","Configuration saved in outputs/checkpoint-500/config.json\n","Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 256\n","The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 9119\n","  Batch size = 256\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["cv_pearson_dbt = []\n","\n","for i in range(n_folds):\n","    ds_cv = get_fold(folds, i, model_dbt)\n","    trainer_cv = get_trainer(ds_cv, model_dbt);\n","    trainer_cv.train()\n","    metrics = [o['eval_pearson'] \n","           for o in trainer_cv.state.log_history\n","           if 'eval_pearson' in o]\n","    cv_pearson_dbt.append(metrics[-1])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2svh7s2dBvW9","outputId":"36ef0149-f361-4f60-c122-6e0f878891f5"},"outputs":[{"data":{"text/plain":["[0.7998729789149337,\n"," 0.7894370591127519,\n"," 0.7879995197728323,\n"," 0.8003102122402879]"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["cv_valid_loss"]},{"cell_type":"markdown","metadata":{"id":"mALFle9xba5l"},"source":["## Bert For Patent"]},{"cell_type":"markdown","metadata":{"id":"wsJ1Y79O-N7h"},"source":["According to the blog post \"How AI, and specifically BERT, helps the patent industry\": Google trained a BERT (bidirectional encoder representation from transformers) model on over 100 million patent publications from the U.S. and other countries using open-source tooling. Google’s release of the BERT model (paper, blog post, and open-source code) in 2018 was an important breakthrough that leveraged transformers to outperform other leading state of the art models across major NLP benchmarks, including GLUE, MultiNLI, and SQuAD. Shortly after its release, the BERT framework and many additional transformer-based extensions gained widespread industry adoption across domains like search, chatbots, and translation."]},{"cell_type":"markdown","metadata":{"id":"1b-8x5OC-N7i"},"source":["We repeat the same process (as we did for DeBERTa):initate and train the model, then get its predictions on the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6wtXzsUk0py","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["57c0f188e16743b5ab82dc2fa0c9e425","172737f1710e47ce9769d0660084465b","db11a43dc2ff4784811e0a43a0dc6b07","577481543a0b4e4c81cd243e243031c1","9cf0bc50a5994c25ba4f439372b196e8","cad94cf7fe454dffbd7b113ea25a2dd5","a750a74c274e40b4aa8d81c3fdbab30c","48bbe0de4ffe4e77967c51a3d243d61e","caa7295c24e34eb2a5ed9d92a08870b2","0c4943b177cc4be4941873114109f96a","447e9fd380ad4eb0bb283ac7ee5b53a9","24b4d1cd437e4a8095764d0882100000","0fe096db40704a62a44c1b25e45cdee4","5cb714c34fd44717b30a17a67d85a791","425a0d9981c546b08e09dafdeaa12538","a43b755b4af14a37a9bd7a3995f513a3","4c80ceb47fd942b0bc8594fa8c8506c0","e89f54c77494438f9d1abd39632960b7","e1f13b1ffd594c9da39b9b5d3fdb70ce","8a6e6ef6f4374607ae31bfdb33fe429c","f970f165ac4244bca9944bca03d7e15e","f07f9b83dff840838eb2701d844dda28","23e085bb9e5b4cc38a31e8c35ccfe962","8a94554a537b43e7b7430b84128e922d","5fb7e00a921f435f8a5a036276055316","b6f53cc8258c431a937fb5d71aa1e97d","0b730c6e787a4e2fb23129018d400c7c","f2f0d6adf9644f0f9c5ce3c8a2aa843d","87a0f298f1214e8a9f737d57c56e096f","e5c50177cd02434b8b94884108d5e09e","74bcce6290284c75a9eae8257661d70e","905f8985eda04d64abb24bbb419f1838","1d920ed4d1544529a85f32f731b07f00","fa77ba0335594e1a9df91ab7661485de","d6475485ea2e4e439f4c9b4465bc751d","5bfbb18d8d8e45899e8ad4e8f2ce74eb","648b5a53badf407b8a08a98d2423c604","df5ba81dab0741c9bab303d35ef28c20","182ba9bd0ad448bea245db0e5743269d","135280ea85564777be1fb3503dc9e307","9072e407e47d435190e48a6710b76548","26479cc813fe427e8555dfaa421fc495","00ea039bf94a40bfa7bff1c9236f990e","05866f5e62594f42b9591de6897b5f74"]},"executionInfo":{"status":"error","timestamp":1669232101798,"user_tz":420,"elapsed":91025,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}},"outputId":"3aef2f98-f8fe-4c53-f5e0-831e496ea5f3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/327 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c0f188e16743b5ab82dc2fa0c9e425"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/329k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b4d1cd437e4a8095764d0882100000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/37 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e085bb9e5b4cc38a31e8c35ccfe962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.38G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa77ba0335594e1a9df91ab7661485de"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at anferico/bert-for-patents and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using cuda_amp half precision backend\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 27889\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 872\n","  Number of trainable parameters = 344704001\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='219' max='872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [219/872 00:35 < 01:46, 6.11 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [34/34 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 8584\n","  Batch size = 256\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-d5e012889738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdds_bfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer_bfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdds_bfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer_bfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2087\u001b[0m                     )\n\u001b[1;32m   2088\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2801\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m         )\n\u001b[1;32m   2805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3079\u001b[0m                 )\n\u001b[1;32m   3080\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-4ab4f0999860>\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pearson'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[1;32m   2681\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   2682\u001b[0m                       DeprecationWarning, stacklevel=3)\n\u001b[0;32m-> 2683\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2684\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcov\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mddof\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 8584"]}],"source":["dds_bfp = get_dds(df, model_bfp)\n","trainer_bfp = get_trainer(dds_bfp, model_bfp)\n","trainer_bfp.train()"]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpIuj5QeAyAP","executionInfo":{"status":"ok","timestamp":1669232574150,"user_tz":420,"elapsed":5,"user":{"displayName":"Dongdong Lu","userId":"11143935684029678426"}},"outputId":"88fe7c5c-01b3-4f49-8e16-44001abb2dab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['id', 'anchor', 'target', 'context', 'score', 'section', 'inputs'], dtype='object')"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVRDbWq3-N7i"},"outputs":[],"source":["ds_test_bfp = dds_bfp['test']\n","ds_test_bfp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBTE5ynqk0pz"},"outputs":[],"source":["def get_test_predictions(ds, trainer):\n","    preds= trainer.predict(ds)\n","    preds_values = preds.predictions\n","    preds_values_clipped = np.clip(preds_values, 0, 1)\n","    return preds_values_clipped\n","\n","%%capture --no-display\n","get_test_predictions(ds_test_bfp, trainer_dbt)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"_tAkQfXF-N7i"},"source":["### Error Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lb0BPvAK-N7i"},"outputs":[],"source":["df = dds['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kz4OhXt_-N7i"},"outputs":[],"source":["df = df.add_column('preds', preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGQT3eu_iJlT"},"outputs":[],"source":["df.set_format('pandas')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PI5in2Xb-N7i"},"outputs":[],"source":["df_loss = abs(df['label'] - df['preds'])\n","df = df.add_column('loss', df_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbxCV8H2jMnU"},"outputs":[],"source":["df['loss']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZ1E7Pjpl5tT"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCspjlCDmUsK"},"outputs":[],"source":["# officially change the format to pandas df\n","ds_train = df[:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPPdYQAQmYkL"},"outputs":[],"source":["ds_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faXYWT2X-N7j"},"outputs":[],"source":["ds_train = ds_train.sort_values(\"loss\", ascending=False)[['input_ids', 'label', 'preds', 'loss']]\n","ds_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbPK-dyvtkp-"},"outputs":[],"source":["ds_train['inputs'] = ds_train['input_ids'].map(lambda x: tokz.convert_tokens_to_string(tokz.convert_ids_to_tokens(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjaEUol5-N7j"},"outputs":[],"source":["# eyeball some big losses come from mis-labels or controvosial labels\n","ds_train[['inputs', 'label', 'preds', 'loss']][:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzJ-ayP--N7j"},"outputs":[],"source":["def round_to_quarter(x):\n","  x_times4 = 4 * x\n","  x_time4_rounded = round(x_times4)\n","  x_rtq = x_time4_rounded / 4\n","  return x_rtq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xc5EwzQKiimf"},"outputs":[],"source":["round_to_quarter(0.37), round_to_quarter(0.375)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lZ7P-xrxSgf"},"outputs":[],"source":["ds_train['y_preds'] = ds_train['preds'].map(lambda x: round_to_quarter(x))\n","ds_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tX1udRj_-N7j"},"outputs":[],"source":["# extract predictions and labels\n","y_preds = ds_train['y_preds']\n","y_valid = ds_train['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-A4o6Fo-N7k"},"outputs":[],"source":["# convert the values into strings \n","# so that they can be used as inputs for the confusion matrix\n","y_preds = [str(y_pred) for y_pred in y_preds]\n","y_valid = list(map(str, y_valid))\n","\n","y_preds[:5], y_valid[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb4wtct7-N7k"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","\n","labels = [0, 0.25, 0.5, 0.75, 1]\n","\n","\n","def plot_confusion_matrix(y_preds, y_true, labels):\n","    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","    fig, ax = plt.subplots(figsize=(5, 5))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","    plt.title(\"Normalized confusion matrix\")\n","    plt.show()\n","    \n","\n","\n","plot_confusion_matrix(y_preds, y_valid, labels)"]},{"cell_type":"markdown","metadata":{"id":"CNPtMRGkl9du"},"source":["# Combine Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFfrj6o6l-5t"},"outputs":[],"source":["models = [model_dbt, model_bfp]\n","\n","trainers = [trainer_dbt, trainer_bfp]\n","\n","predictions = []\n","\n","for model, trainer in zip(models, trainers):\n","    test_ds_ems = get_dds(model)\n","    preds = get_predictions(test_ds_ems, trainer)\n","    predictions.append(preds_clipped)\n","    # take average here.\n"]},{"cell_type":"markdown","metadata":{"id":"uMYxIgkJzmT3"},"source":["# Try Model Variations"]},{"cell_type":"markdown","metadata":{"id":"WgSLDiRK-pz_"},"source":["## Change special tokens"]},{"cell_type":"markdown","metadata":{"id":"9iiE350u-_OC","tags":[]},"source":["### Use patent section letters as sep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Htx4_z2Fh-Qq"},"outputs":[],"source":["df.section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pXxvOaH-tic"},"outputs":[],"source":["df['sectok'] = '[' + df.section + ']'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFKmUTAS_URu"},"outputs":[],"source":["sectoks =  list(df.sectok.unique())\n","sectoks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Q-FvQWx_fe-"},"outputs":[],"source":["tokz.add_special_tokens({'additional_special_tokens': sectoks})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CeM6mjq_gIe"},"outputs":[],"source":["df['inputs'] = df['sectok'] + sep + df['context'] + sep + df['anchor'].str.lower() + sep + df.target\n","df.inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrbfV73Y_1wl"},"outputs":[],"source":["dds = get_dds(df)\n","dds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXf0AHY5_2Wl"},"outputs":[],"source":["model_patent_section = get_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5xaLmcDAz_m"},"outputs":[],"source":["model_patent_section.resize_token_embeddings(len(tokz))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BzgMIBjA9qm"},"outputs":[],"source":["trainer_patent_section =  get_trainer(dds, model=model_patent_section)\n","trainer_patent_section.train()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["9_QsG-oPvX0a","xam9HEiPnnVV","v27-Cg0Ov9Ib","RQdRy6-WbQ70","8JoO9sIAX09h","r5L8h2mR_JP6","mALFle9xba5l","_tAkQfXF-N7i","CNPtMRGkl9du","uMYxIgkJzmT3","WgSLDiRK-pz_","9iiE350u-_OC"]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"424078f2bd6d47589ed69d49227c7181":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba9fc9e390a84126a9a1bc56c790b704","IPY_MODEL_57a274a367fa47adb6a5ce07e4262370","IPY_MODEL_d9222d649b3f453eba6937d4311914f5"],"layout":"IPY_MODEL_d263504aef444faabb8cf50164d310c3"}},"57a274a367fa47adb6a5ce07e4262370":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5eef78f8fe7b45cebfa17dd7bed5b23e","max":37,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f247415d8a034ab59f463c24fab8f9d0","value":37}},"58c08bfc34e34fd1a4273c75c88558d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"594a78b41eb34ac8b1c6df5827495566":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eef78f8fe7b45cebfa17dd7bed5b23e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91296adfaf59460985ad9d2686a0429e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba9fc9e390a84126a9a1bc56c790b704":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0453807745f4b28819efa511d3e5eae","placeholder":"​","style":"IPY_MODEL_58c08bfc34e34fd1a4273c75c88558d4","value":"100%"}},"d263504aef444faabb8cf50164d310c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9222d649b3f453eba6937d4311914f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_594a78b41eb34ac8b1c6df5827495566","placeholder":"​","style":"IPY_MODEL_91296adfaf59460985ad9d2686a0429e","value":" 37/37 [00:01&lt;00:00, 18.73ba/s]"}},"f0453807745f4b28819efa511d3e5eae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f247415d8a034ab59f463c24fab8f9d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"792e38dd4c764bcdb79bce27c758a83c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef306eea21524f91929322f97384b22a","IPY_MODEL_e8e5ebf2cf6c4b5894aaa48682560fc5","IPY_MODEL_c7489cf97ad04ad5bbcf27d6865974fc"],"layout":"IPY_MODEL_e9ebefbb7258444dab3e2965a8c486ac"}},"ef306eea21524f91929322f97384b22a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fef12060d9e6465aa7ce0c067d4e64dd","placeholder":"​","style":"IPY_MODEL_52aa6bc29fc44c3bb3e2f4ef95fafe01","value":"Downloading: 100%"}},"e8e5ebf2cf6c4b5894aaa48682560fc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2092d0e304b24b0a953c455a821a64a1","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_692d36d1936d4f90bcffc7f0949a2334","value":52}},"c7489cf97ad04ad5bbcf27d6865974fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86f93ada9e704297be87a9be952b7e0f","placeholder":"​","style":"IPY_MODEL_b251ae39e8dd4a4d896e2588966c3314","value":" 52.0/52.0 [00:00&lt;00:00, 1.90kB/s]"}},"e9ebefbb7258444dab3e2965a8c486ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fef12060d9e6465aa7ce0c067d4e64dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52aa6bc29fc44c3bb3e2f4ef95fafe01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2092d0e304b24b0a953c455a821a64a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"692d36d1936d4f90bcffc7f0949a2334":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86f93ada9e704297be87a9be952b7e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b251ae39e8dd4a4d896e2588966c3314":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0b5e9b4eb2043c7aeced5a78e47da05":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afaf0d67c4d14862b32989c045866cbc","IPY_MODEL_2d5207251012429784a30933feaaa21f","IPY_MODEL_937badeda02b43f88ebf4be4e1ffcf31"],"layout":"IPY_MODEL_b2bb22af62af46ceb462503b62150c7f"}},"afaf0d67c4d14862b32989c045866cbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8d08cc38b724d6d96328a912396e36e","placeholder":"​","style":"IPY_MODEL_4f777f397c5d43369f5cf1918d1630ad","value":"Downloading: 100%"}},"2d5207251012429784a30933feaaa21f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fb0d0ae05974242a26fb475a65b3a81","max":578,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c883b638ac644600a267d2e994db408d","value":578}},"937badeda02b43f88ebf4be4e1ffcf31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2525e1c4d4f34069bfa446ff91706c0b","placeholder":"​","style":"IPY_MODEL_5087c845d3614c2cb8e4745cb85d0f7f","value":" 578/578 [00:00&lt;00:00, 20.8kB/s]"}},"b2bb22af62af46ceb462503b62150c7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8d08cc38b724d6d96328a912396e36e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f777f397c5d43369f5cf1918d1630ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fb0d0ae05974242a26fb475a65b3a81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c883b638ac644600a267d2e994db408d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2525e1c4d4f34069bfa446ff91706c0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5087c845d3614c2cb8e4745cb85d0f7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c99f7e6aa04b4432897585737ed4cac5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0085dfac9c6041509b69c35c4bb17425","IPY_MODEL_26848567776c43ffb18dc063a3d38139","IPY_MODEL_5124988af1024c53bb2971f2f2618abc"],"layout":"IPY_MODEL_e9eab37ab65f4708bc7049bb46cd4842"}},"0085dfac9c6041509b69c35c4bb17425":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41132e9d6c8c46229fbf2b9b9d1466cc","placeholder":"​","style":"IPY_MODEL_1e8316b6aa764f5abf32555b3a6664ae","value":"Downloading: 100%"}},"26848567776c43ffb18dc063a3d38139":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b78ae07c3d9445b599cb7b5c3f704e8f","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b163b22fce8e45d18f3edc289bce2482","value":2464616}},"5124988af1024c53bb2971f2f2618abc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ec9dbcf0e78438e99177422362e506d","placeholder":"​","style":"IPY_MODEL_b18673fdf4d14346bf7d2704b8d8d59e","value":" 2.46M/2.46M [00:00&lt;00:00, 32.9MB/s]"}},"e9eab37ab65f4708bc7049bb46cd4842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41132e9d6c8c46229fbf2b9b9d1466cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e8316b6aa764f5abf32555b3a6664ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b78ae07c3d9445b599cb7b5c3f704e8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b163b22fce8e45d18f3edc289bce2482":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ec9dbcf0e78438e99177422362e506d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18673fdf4d14346bf7d2704b8d8d59e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eebd933cf23e4c91ad5147d9f0550b3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e35a29a85d74501b8368afd86180485","IPY_MODEL_ba186bacf97d4047917fa7b5fc4fc442","IPY_MODEL_a648959982bb4d6cb01df889fc3f9a3b"],"layout":"IPY_MODEL_cf9c356914d54264ab34e1fbfa635ae7"}},"0e35a29a85d74501b8368afd86180485":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a988e5d02fd498eba93e787ee35be62","placeholder":"​","style":"IPY_MODEL_379f4258dcb64108bc73aee4ab0e9524","value":"100%"}},"ba186bacf97d4047917fa7b5fc4fc442":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0459045a6ea4e3b9255b545b68e0864","max":37,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ee76b2b907844378616db3ee22851f3","value":37}},"a648959982bb4d6cb01df889fc3f9a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3af7688d94464e8b8bbed6c1cdd1bdd7","placeholder":"​","style":"IPY_MODEL_d52bc4cbd1854df686c6e8f224c9f692","value":" 37/37 [00:01&lt;00:00, 33.27ba/s]"}},"cf9c356914d54264ab34e1fbfa635ae7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a988e5d02fd498eba93e787ee35be62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"379f4258dcb64108bc73aee4ab0e9524":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0459045a6ea4e3b9255b545b68e0864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee76b2b907844378616db3ee22851f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3af7688d94464e8b8bbed6c1cdd1bdd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52bc4cbd1854df686c6e8f224c9f692":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57c0f188e16743b5ab82dc2fa0c9e425":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_172737f1710e47ce9769d0660084465b","IPY_MODEL_db11a43dc2ff4784811e0a43a0dc6b07","IPY_MODEL_577481543a0b4e4c81cd243e243031c1"],"layout":"IPY_MODEL_9cf0bc50a5994c25ba4f439372b196e8"}},"172737f1710e47ce9769d0660084465b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cad94cf7fe454dffbd7b113ea25a2dd5","placeholder":"​","style":"IPY_MODEL_a750a74c274e40b4aa8d81c3fdbab30c","value":"Downloading: 100%"}},"db11a43dc2ff4784811e0a43a0dc6b07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48bbe0de4ffe4e77967c51a3d243d61e","max":327,"min":0,"orientation":"horizontal","style":"IPY_MODEL_caa7295c24e34eb2a5ed9d92a08870b2","value":327}},"577481543a0b4e4c81cd243e243031c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c4943b177cc4be4941873114109f96a","placeholder":"​","style":"IPY_MODEL_447e9fd380ad4eb0bb283ac7ee5b53a9","value":" 327/327 [00:00&lt;00:00, 11.4kB/s]"}},"9cf0bc50a5994c25ba4f439372b196e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cad94cf7fe454dffbd7b113ea25a2dd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a750a74c274e40b4aa8d81c3fdbab30c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48bbe0de4ffe4e77967c51a3d243d61e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caa7295c24e34eb2a5ed9d92a08870b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c4943b177cc4be4941873114109f96a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447e9fd380ad4eb0bb283ac7ee5b53a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24b4d1cd437e4a8095764d0882100000":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fe096db40704a62a44c1b25e45cdee4","IPY_MODEL_5cb714c34fd44717b30a17a67d85a791","IPY_MODEL_425a0d9981c546b08e09dafdeaa12538"],"layout":"IPY_MODEL_a43b755b4af14a37a9bd7a3995f513a3"}},"0fe096db40704a62a44c1b25e45cdee4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c80ceb47fd942b0bc8594fa8c8506c0","placeholder":"​","style":"IPY_MODEL_e89f54c77494438f9d1abd39632960b7","value":"Downloading: 100%"}},"5cb714c34fd44717b30a17a67d85a791":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1f13b1ffd594c9da39b9b5d3fdb70ce","max":329241,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a6e6ef6f4374607ae31bfdb33fe429c","value":329241}},"425a0d9981c546b08e09dafdeaa12538":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f970f165ac4244bca9944bca03d7e15e","placeholder":"​","style":"IPY_MODEL_f07f9b83dff840838eb2701d844dda28","value":" 329k/329k [00:00&lt;00:00, 265kB/s]"}},"a43b755b4af14a37a9bd7a3995f513a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c80ceb47fd942b0bc8594fa8c8506c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89f54c77494438f9d1abd39632960b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1f13b1ffd594c9da39b9b5d3fdb70ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a6e6ef6f4374607ae31bfdb33fe429c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f970f165ac4244bca9944bca03d7e15e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f07f9b83dff840838eb2701d844dda28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23e085bb9e5b4cc38a31e8c35ccfe962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a94554a537b43e7b7430b84128e922d","IPY_MODEL_5fb7e00a921f435f8a5a036276055316","IPY_MODEL_b6f53cc8258c431a937fb5d71aa1e97d"],"layout":"IPY_MODEL_0b730c6e787a4e2fb23129018d400c7c"}},"8a94554a537b43e7b7430b84128e922d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2f0d6adf9644f0f9c5ce3c8a2aa843d","placeholder":"​","style":"IPY_MODEL_87a0f298f1214e8a9f737d57c56e096f","value":"100%"}},"5fb7e00a921f435f8a5a036276055316":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5c50177cd02434b8b94884108d5e09e","max":37,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74bcce6290284c75a9eae8257661d70e","value":37}},"b6f53cc8258c431a937fb5d71aa1e97d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_905f8985eda04d64abb24bbb419f1838","placeholder":"​","style":"IPY_MODEL_1d920ed4d1544529a85f32f731b07f00","value":" 37/37 [00:01&lt;00:00, 25.72ba/s]"}},"0b730c6e787a4e2fb23129018d400c7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2f0d6adf9644f0f9c5ce3c8a2aa843d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a0f298f1214e8a9f737d57c56e096f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5c50177cd02434b8b94884108d5e09e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74bcce6290284c75a9eae8257661d70e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"905f8985eda04d64abb24bbb419f1838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d920ed4d1544529a85f32f731b07f00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa77ba0335594e1a9df91ab7661485de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6475485ea2e4e439f4c9b4465bc751d","IPY_MODEL_5bfbb18d8d8e45899e8ad4e8f2ce74eb","IPY_MODEL_648b5a53badf407b8a08a98d2423c604"],"layout":"IPY_MODEL_df5ba81dab0741c9bab303d35ef28c20"}},"d6475485ea2e4e439f4c9b4465bc751d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_182ba9bd0ad448bea245db0e5743269d","placeholder":"​","style":"IPY_MODEL_135280ea85564777be1fb3503dc9e307","value":"Downloading: 100%"}},"5bfbb18d8d8e45899e8ad4e8f2ce74eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9072e407e47d435190e48a6710b76548","max":1383349812,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26479cc813fe427e8555dfaa421fc495","value":1383349812}},"648b5a53badf407b8a08a98d2423c604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00ea039bf94a40bfa7bff1c9236f990e","placeholder":"​","style":"IPY_MODEL_05866f5e62594f42b9591de6897b5f74","value":" 1.38G/1.38G [00:22&lt;00:00, 61.5MB/s]"}},"df5ba81dab0741c9bab303d35ef28c20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182ba9bd0ad448bea245db0e5743269d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"135280ea85564777be1fb3503dc9e307":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9072e407e47d435190e48a6710b76548":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26479cc813fe427e8555dfaa421fc495":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00ea039bf94a40bfa7bff1c9236f990e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05866f5e62594f42b9591de6897b5f74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}